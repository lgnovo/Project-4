# -*- coding: utf-8 -*-
"""Beyonka_Project4_Prdiction1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JLwZXxrNnqTUkVtSmP0782cYo3C4eFdW
"""

import os
# Find the latest version of spark 3.2 from http://www.apache.org/dist/spark/ and enter as the spark version
# For example:
spark_version = 'spark-3.4.3'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark

# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop3"

# Start a SparkSession
import findspark
findspark.init()


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, when
spark = SparkSession.builder.appName("BankChurners").getOrCreate()

# Read in data from S3 Buckets
from pyspark import SparkFiles

url = "https://groupfourproject.s3.ca-central-1.amazonaws.com/bank_churners.csv"
spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("bank_churners.csv"), header=True, sep=',', inferSchema=True)

# Show the DataFrame
df.show()

#Displaying Column Type
df.printSchema()

#Dropping duplicates within the dataframe
df.dropDuplicates().show()

#Drooping NA in the dataframe
df.na.drop(how='all')

#Display DataFrame
df.show()

#Dropping columns in DataFrame
credit_df= df.drop('CLIENTNUM', 'Customer_Age', 'Marital_Status',
'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',
'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',
'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1')

#Displaying DataFrame
credit_df.show()

#Create a temporary view
credit_df.createOrReplaceTempView("bank")

"""**Demographic Data to Analyze**

**Gender**
"""

gender = spark.sql("""
SELECT
  Attrition_Flag as Flag,
  Gender,
  COUNT(Attrition_Flag) as Count
FROM
  bank
WHERE
  Gender IN ('F', 'M')
GROUP BY
  Attrition_Flag,
  Gender
""")

gender.show()

gendercount = spark.sql("""
SELECT
  Attrition_Flag as Flag,
  COUNT(*) as Count
FROM
  bank
WHERE
  Gender IN ('F', 'M')
GROUP BY
  Attrition_Flag
""")

gendercount.show()

"""**Dependent Count**"""

dependent_count= spark.sql("""
SELECT
  Attrition_Flag as Flag,
  COUNT(*) as DependentCount,
  CASE
    WHEN Dependent_count BETWEEN 0 AND 5 THEN '0-5'
    ELSE 'Other'
  END AS DependentRange
FROM
  bank
GROUP BY
  Attrition_Flag,
  CASE
    WHEN Dependent_count BETWEEN 0 AND 5 THEN '0-5'
    ELSE 'Other'
  END
ORDER BY
  DependentRange
""")
dependent_count.show()

"""**Education**"""

education = spark.sql("""
SELECT
  Attrition_Flag as Flag,
  Education_Level as Education,
  COUNT(Education_Level) as EducationCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Education_Level
ORDER BY
  Flag
""")
education.show()

"""**Income Category**"""

income = spark.sql("""
SELECT
  Attrition_Flag as Flag,
  Income_Category as Income,
  COUNT(*) as IncomeCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Income_Category
ORDER BY
  Income
""")

income.show()

"""**Behavioral (Bank) Data**

**Months on book**
"""

month_book = spark.sql("""SELECT
  Attrition_Flag as Flag,
  Months_on_book as Months,
  COUNT(*) as MonthsCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Months_on_book
ORDER BY
 Months
""")
month_book.show()

"""**Total Relationsip Count**"""

total_relationship = spark.sql("""SELECT
  Attrition_Flag as Flag,
  Total_Relationship_Count as TotalRelationship,
  COUNT(*) as MonthsCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Total_Relationship_Count
ORDER BY
  TotalRelationship
""")
total_relationship.show()

"""**Months Inactive 12 months**"""

months_inactive = spark.sql("""SELECT
  Attrition_Flag as Flag,
  Months_Inactive_12_mon as MonthsInactive,
  COUNT(*) as InactiveCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Months_Inactive_12_mon
ORDER BY
  MonthsInactive
""")
months_inactive.show()

"""**Contacts Count 12 months**"""

contacts = spark.sql("""SELECT
  Attrition_Flag as Flag,
  Contacts_Count_12_mon as Contact,
  COUNT(*) as MonthsCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Contacts_Count_12_mon
ORDER BY
  Contact
""")
contacts.show()

"""**Credit Limit**"""

credit = spark.sql("""SELECT
  Attrition_Flag as Flag,
  Credit_Limit as CreditLimit,
  COUNT(*) as CreditCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Credit_Limit
ORDER BY
  CreditCount
""")
credit.show()

"""**Total Trans Amt**"""

total_amt = spark.sql("""SELECT
  Attrition_Flag as Flag,
  Total_Trans_Amt as TotalTrans,
  COUNT(*) as TransCount
FROM
  bank
GROUP BY
  Attrition_Flag,
  Total_Trans_Amt
ORDER BY
  TotalTrans
""")
total_amt.show()

"""**Total Trans Count**"""

total_count = spark.sql("""SELECT
  Attrition_Flag as Flag,
  Total_Trans_Ct as TotalTransCount,
  COUNT(*) as CountTotalTrans
FROM
  bank
GROUP BY
  Attrition_Flag,
  Total_Trans_Ct
ORDER BY
  CountTotalTrans
""")
total_count.show()

from pyspark.sql import functions as F
from pyspark.sql import types as T


#Gives a label to each category
#creates onehot encoding vector
from pyspark.ml.feature import StringIndexer, OneHotEncoder

#Used to creat vector from features and modeling takes vector as an input
from pyspark.ml.feature import VectorAssembler

#Used for classification problems
from pyspark.ml.classification import DecisionTreeClassifier

#Maniplating Attrition_Flag to 0= Attrited Customer and 1= Existing Customer
attrition_df = credit_df.withColumn("Attrition_Flag", F.when(F.col("Attrition_Flag") =='Attrited Customer', 0).otherwise(1))

#Displaying DataFrame
attrition_df.show(3, truncate=False)

#Displaying count of Attrition_Flag
attrition_df.groupby("Attrition_Flag").count().show()

"""**1st Modeling**"""

(attrited_train_df, attrited_test_df) = attrition_df.randomSplit([0.8, 0.2], 11)
print("Number of train samples: " + str(attrited_train_df.count()))
print("Number of test samples: " + str(attrited_test_df.count()))

# Apply StringIndexer to the Attrition_Flag column
attrition_indexer = StringIndexer(inputCol="Attrition_Flag", outputCol="Attrition_Flag_Index")
attrition_indexer = attrition_indexer.fit(attrited_train_df)
attrited_train_df = attrition_indexer.transform(attrited_train_df)

# Show the transformed DataFrame
attrited_train_df.show(3, truncate=False)

attrited_inputCols=[
 'Dependent_count',
 'Months_on_book',
 'Total_Relationship_Count',
 'Months_Inactive_12_mon',
 'Contacts_Count_12_mon',
 'Credit_Limit',
 'Total_Revolving_Bal',
 'Avg_Open_To_Buy',
 'Total_Amt_Chng_Q4_Q1',
 'Total_Trans_Amt',
 'Total_Trans_Ct',
 'Total_Ct_Chng_Q4_Q1',
 'Avg_Utilization_Ratio'
    ]

attrited_outputCol="features"

vector_assembler = VectorAssembler(inputCols= attrited_inputCols, outputCol=attrited_outputCol )
attrited_train_df = vector_assembler.transform(attrited_train_df)

attrited_train_df.show(3, truncate=False)

#Modeling dataframe for Attrition_Flag
attrited_modeling_df = attrited_train_df.select('features', 'Attrition_Flag')

#Displaying DataFrame
attrited_modeling_df.show()

#Creating a DecisionTreeClassifier model
model_attrition = DecisionTreeClassifier(labelCol="Attrition_Flag", featuresCol="features")


#Train model with traing data
model_attrition = model_attrition.fit(attrited_modeling_df)

predicton = model_attrition.transform(attrited_modeling_df)
predicton.show()

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(labelCol="Attrition_Flag")
area = evaluator.evaluate(predicton)

print(area)

"""**1st Test Prediction**"""

#Test data- transform test data using all tranformers and estimators in order

attrited_test_df = attrition_indexer.transform(attrited_test_df)
attrited_test_df = vector_assembler.transform(attrited_test_df)
test_predications =  model_attrition.transform(attrited_test_df)

test_predications.show(3, truncate=False)

area = evaluator.evaluate(test_predications)

print(area)