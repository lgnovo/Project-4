# -*- coding: utf-8 -*-
"""Beyonka_Project4_Prdiction2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ClbTx5dRv8CC6x24gzoprMQIB7V4tbd
"""

import os
# Find the latest version of spark 3.2 from http://www.apache.org/dist/spark/ and enter as the spark version
# For example:
spark_version = 'spark-3.5.1'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark

# Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop3"

# Start a SparkSession
import findspark
findspark.init()

from pyspark.sql import functions as F
from pyspark.sql import types as T


#Gives a label to each category
#creates onehot encoding vector
from pyspark.ml.feature import StringIndexer, OneHotEncoder

#Used to creat vector from features and modeling takes vector as an input
from pyspark.ml.feature import VectorAssembler

#Used for classification problems
from pyspark.ml.classification import DecisionTreeClassifier

# Start Spark session
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, when
spark = SparkSession.builder.appName("BankChurners").getOrCreate()

# Read in data from S3 Buckets
from pyspark import SparkFiles
url = "https://groupfourproject.s3.ca-central-1.amazonaws.com/bank_churners.csv"
spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("bank_churners.csv"), header=True, sep=',', inferSchema=True)


# Show DataFrame
df.show()

#Dropping duplicates within the dataframe
df.dropDuplicates().show()

credit_df= df.drop('CLIENTNUM', 'Customer_Age', 'Marital_Status',
'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',
'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',
'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1')

credit_df.show()

#Maniplating Attrition_Flag to 0= Attrited Customer and 1= Existing Customer
attrition_df = credit_df.withColumn("Attrition_Flag", F.when(F.col("Attrition_Flag") =='Attrited Customer', 0).otherwise(1))

#Displaying DataFrame
attrition_df.show(3, truncate=False)

#Displaying count of Attrition_Flag
attrition_df.groupby("Attrition_Flag").count().show()

from pyspark.sql import functions as F

# Perform groupBy and count operation on 'Attrition_Flag' column
attrition_flag_counts = credit_df.groupBy("Attrition_Flag").count()

# Join the original DataFrame with the DataFrame containing counts
updated_df = credit_df.join(attrition_flag_counts, "Attrition_Flag", "left")

df_8500 = updated_df.filter(updated_df["count"]==8500)
df_sample =df_8500.sample(fraction=1627.0/8500.0, seed=1)

df_1627 = updated_df.filter(updated_df["count"] !=8500)

# Combine the sampled data with the original data
updated_df = df_sample.union(df_1627)

# Show the updated DataFrame
updated_df.show()

updated_df.tail(10)
updated_df.show()

new_attrited_df = updated_df.withColumn("Attrition_Flag", F.when(F.col("Attrition_Flag") =='Attrited Customer', 0).otherwise(1))
new_attrited_df.show(3, truncate=False)

new_attrited_df.groupby("Attrition_Flag").count().show()

"""**2nd Modeling**"""

(train_df_2, test_df_2) = new_attrited_df.randomSplit([0.8, 0.2], 11)
print("Number of train samples: " + str(train_df_2.count()))
print("Number of test samples: " + str(test_df_2.count()))

# Apply StringIndexer to the Attrition_Flag column
attrition_indexer_2 = StringIndexer(inputCol="Attrition_Flag", outputCol="Attrition_Flag_Index")
attrition_indexer_2 = attrition_indexer_2.fit(train_df_2)
attrited_train_df_2 = attrition_indexer_2.transform(train_df_2)

# Show the transformed DataFrame
attrited_train_df_2.show(3, truncate=False)

inputCols_2=[
 'Dependent_count',
 'Months_on_book',
 'Total_Relationship_Count',
 'Months_Inactive_12_mon',
 'Contacts_Count_12_mon',
 'Credit_Limit',
 'Total_Revolving_Bal',
 'Avg_Open_To_Buy',
 'Total_Amt_Chng_Q4_Q1',
 'Total_Trans_Amt',
 'Total_Trans_Ct',
 'Total_Ct_Chng_Q4_Q1',
 'Avg_Utilization_Ratio'
    ]

outputCol_2="features"

vector_assembler_2 = VectorAssembler(inputCols= inputCols_2, outputCol=outputCol_2)
train_df2 = vector_assembler_2.transform(train_df_2)

train_df2.show(3, truncate=False)

#Modeling dataframe for Attrition_Flag
modeling_df_2 = train_df2.select(['features', 'Attrition_Flag'])

#Displaying top DataFrame
modeling_df_2.show()

#Displaying bottom DataFrame
modeling_df_2.tail(10)
modeling_df_2.show()

#Creating a DecisionTreeClassifier model
model_attrition_2 = DecisionTreeClassifier(labelCol="Attrition_Flag", featuresCol="features")

#Train model
model_attrition_2 = model_attrition_2.fit(modeling_df_2)

predicton = model_attrition_2.transform(modeling_df_2)
predicton.show()

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(labelCol="Attrition_Flag")
area = evaluator.evaluate(predicton)

print(area)

"""**2nd Test Prediction**"""

test_df_2 = attrition_indexer_2 .transform(test_df_2)
test_df_2 = vector_assembler_2.transform(test_df_2)
test_predications = model_attrition_2.transform(test_df_2)

test_predications.show(3, truncate=False)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(labelCol="Attrition_Flag")
area = evaluator.evaluate(test_predications)

print(area)